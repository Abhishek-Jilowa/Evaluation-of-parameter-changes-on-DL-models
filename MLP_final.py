# -*- coding: utf-8 -*-
"""DC_MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZMB-NeIVGJSXZaoMlhdnZ2oxGLdwMIqq
"""

from google.colab import drive
drive.mount('/content/drive')

"""# 1. Import necessary libraries <a class="anchor" id="2"></a>


"""

# Commented out IPython magic to ensure Python compatibility.
 

import numpy as np 
import pandas as pd 


import matplotlib.pyplot as plt # plotting library
# %matplotlib inline


from keras.models import Sequential
from keras.layers import Dense , Activation, Dropout,Flatten
from keras.optimizers import Adam ,RMSprop
from keras import  backend as K
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import pickle
from PIL import Image
import torchvision.datasets as dsets
import torchvision.transforms as transforms
import torch
import torch.nn as nn
import pickle
import os
import torch.nn.functional as F
import requests
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sn
import pandas as pd
torch.manual_seed(1)
from sklearn import preprocessing
from torch.utils.data import DataLoader, Dataset, Subset

"""# 2.Importing dataset <a class="anchor" id="3"></a>



"""

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
assert x_train.shape == (50000, 32, 32, 3)
assert x_test.shape == (10000, 32, 32, 3)
assert y_train.shape == (50000, 1)
assert y_test.shape == (10000, 1)

print('Shape of x_train is {}'.format(x_train.shape))
print('Shape of x_test is {}'.format(x_test.shape)) 
print('Shape of y_train is {}'.format(y_train.shape))
print('Shape of y_test is {}'.format(y_test.shape))

"""# 3. Data visualization <a class="anchor" id="4"></a>




- The following code will help to sample the 25 random MNIST digits and visualize them.
"""

import matplotlib.pyplot as plt

cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
print('Example training images and their labels: ' + str([x[0] for x in y_train[0:5]])) 
print('Corresponding classes for the labels: ' + str([cifar_classes[x[0]] for x in y_train[0:5]]))

f, axarr = plt.subplots(1, 5)
f.set_size_inches(16, 6)

for i in range(5):
    img = x_train[i]
    axarr[i].imshow(img)
plt.show()

"""# 4. Designing model architecture using Keras <a class="anchor" id="5"></a>


- The following code shows how to design the MLP model architecture using Keras.

- The first step in designing the model architecture is to import the Keras layers. This can be done as follows:

## 4.1 Import Keras layers <a class="anchor" id="5.1"></a>
"""

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.utils import to_categorical, plot_model

"""## 4.2 Compute the number of labels <a class="anchor" id="5.2"></a>

- Now, the data must be in the correct shape and format. 

- After loading the MNIST dataset, the number of labels is computed as:
"""

# compute the number of labels
num_labels = 10
print(num_labels)

"""## 4.3 One-Hot Encoding <a class="anchor" id="5.3"></a>

- At this point, the labels are in digits format, 0 to 9. 

- This sparse scalar representation of labels is not suitable for the neural network prediction layer that outputs probabilities per class. 

- A more suitable format is called a one-hot vector, a 10-dim vector with all elements 0, except for the index of the digit class. 

- For example, if the label is 2, the equivalent one-hot vector is [0,0,1,0,0,0,0,0,0,0]. The first label has index 0.

- The following lines convert each label into a one-hot vector:

## 4.4 Data Preprocessing <a class="anchor" id="5.4"></a>
"""

# image dimensions (assumed square)
image_size = x_train.shape[1]
input_size = image_size * image_size
input_size

y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Transform images from (32,32,3) to 3072-dimensional vectors (32*32*3)

x_train = np.reshape(x_train,(50000,3072))
x_test = np.reshape(x_test,(10000,3072))
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

# Normalization of pixel values (to [0-1] range)

x_train /= 255
x_test /= 255

"""## 4.5 Setting network parameters <a class="anchor" id="5.5"></a>

- Now, we will set the network parameters as follows:
"""

# network parameters
batch_size = 32
hidden_units = 256
dropout = 0.45

"""- The **batch_size** argument indicates the number of data that we will use for each update of the model parameters.

- **Hidden_units** shows the number of hidden units.

- **Dropout** is the dropout rate

## 4.6 Designing the model architecture <a class="anchor" id="5.6"></a>


- The next step is to design the model architecture. The proposed model is made of three MLP layers. 

- In Keras, an MLP layer is referred to as Dense, which stands for the densely connected layer. 

- Both the first and second MLP layers are identical in nature with 256 units each, followed by relu activation and dropout. 

- 256 units are chosen since 128, 512 and 1,024 units have lower performance metrics. At 128 units, the network converges quickly, but has a lower test accuracy. The added number units for 512 or 1,024 does not increase the test accuracy significantly.

- The main data structure in Keras is the Sequential class, which allows the creation of a basic neural network.

- The Sequential class of the Keras library is a wrapper for the sequential neural network model that Keras offers and can be created in the following way:

`from keras.models import Sequential`

`model = Sequential()`

- The model in Keras is considered as a sequence of layers and each of them gradually “distills” the input data to obtain the desired output.

- In Keras, we can add the required types of layers through the **add()** method.
"""

# model is a MLP with ReLU and dropout after each layer
model = Sequential()
model.add(Flatten(input_shape=x_train.shape[1:]))
model.add(Dense(2349, activation='relu'))
model.add(Dropout(0.2134))
model.add(Dense(2763, activation='relu'))
model.add(Dropout(0.3679))
model.add(Dense(2106, activation='relu'))
model.add(Dropout(0.4261))
model.add(Dense(10, activation='softmax'))

"""- Since a Dense layer is a linear operation, a sequence of Dense layers can only approximate a linear function. 

- The problem is that the MNIST digit classification is inherently a non-linear process. Inserting a relu activation between Dense layers will enable MLPs to model non-linear mappings. 

- relu or Rectified Linear Unit (ReLU) is a simple non-linear function. It allows positive inputs to pass through unchanged while clamping everything else to zero.

## 4.7 View model summary <a class="anchor" id="5.7"></a>

- Keras library provides us **summary()** method to check the model description.
"""

model.summary()

"""- The above listing shows the model summary of the proposed network. It requires a total of 269,322 parameters.

- This is substantial considering that we have a simple task of classifying MNIST digits. So, MLPs are not parameter efficient. 

- The total number of parameters required can be computed as follows:

  - From input to Dense layer: 784 × 256 + 256 = 200,960. 
  
  - From first Dense to second Dense: 256 × 256 + 256 = 65,792. 
  
  - From second Dense to the output layer: 10 × 256 + 10 = 2,570. 
  
  - The total is 200,690 + 65,972 + 2,570 = 269,322.

- Another way of verifying the network is by calling the **plot_model()** method as follows:
"""

plot_model(model, to_file='mlp-mnist.png', show_shapes=True)

"""# 5. Implement MLP model using Keras <a class="anchor" id="6"></a>





- The implementation of MLP model in Keras comprises of three steps:-

  - Compiling the model with the compile() method.
  
  - Training the model with fit() method.
  
  - Evaluating the model performance with evaluate() method.

## 5.1 Compile the model with compile() method <a class="anchor" id="6.1"></a>


- Compilation of model can be done as follows:
"""

from tensorflow.keras.optimizers import SGD
sgd = SGD(learning_rate=0.001)

model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])

"""### Loss function (categorical_crossentropy)

- How far the predicted tensor is from the one-hot ground truth vector is called **loss**.

- In this example, we use **categorical_crossentropy** as the loss function. It is the negative of the sum of the product of the target and the logarithm of the prediction. 

- There are other loss functions in Keras, such as mean_absolute_error and binary_crossentropy. The choice of the loss function is not arbitrary but should be a criterion that the model is learning. 

- For classification by category, categorical_crossentropy or mean_squared_error is a good choice after the softmax activation layer. The binary_crossentropy loss function is normally used after the sigmoid activation layer while mean_squared_error is an option for tanh output.

### Optimization (optimizer adam)

- With optimization, the objective is to minimize the loss function. The idea is that if the loss is reduced to an acceptable level, the model has indirectly learned the function mapping input to output.

- In Keras, there are several choices for optimizers. The most commonly used optimizers are; **Stochastic Gradient Descent (SGD)**, **Adaptive Moments (Adam)** and **Root Mean Squared Propagation (RMSprop)**. 

- Each optimizer features tunable parameters like learning rate, momentum, and decay. 

- Adam and RMSprop are variations of SGD with adaptive learning rates. In the proposed classifier network, Adam is used since it has the highest test accuracy.

### Metrics (accuracy)

- Performance metrics are used to determine if a model has learned the underlying data distribution. The default metric in Keras is loss. 

- During training, validation, and testing, other metrics such as **accuracy** can also be included. 

- **Accuracy** is the percent, or fraction, of correct predictions based on ground truth.

## 5.2 Train the model with fit() method <a class="anchor" id="6.2"></a>
"""

model.fit(x_train, y_train, epochs=30, batch_size=20,validation_data=(x_test,y_test),verbose=2)

"""## 5.3 Evaluating model performance with evaluate() method <a class="anchor" id="6.3"></a>"""

loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
print("\nTest accuracy: %.1f%%" % (100.0 * acc))

num_layers=4
max_acc=acc

"""## 5.4 Getting original weights <a class="anchor" id="6.3"></a>"""

weights = model.get_weights()

print(len(weights))

for i in range(len(weights)):
  print(weights[i].shape)

weights[1]

print(len( model.layers))

for layers in model.layers:
  print(layers.get_weights())

len(weights[1])

original_weights=weights

original_model=model

type(weights[1])

weights=original_weights

weights_backup=original_weights

"""## 5.5 Defining scalars and changing weights of every layer by multipyling those scalars one by one"""

scalars=[ .75, 0.8, .85, 0.9, 1.1,1.25,1.3,1.35,1.4]
new_acc=[]
acc_change=[]

l1=[2*i for i in range(num_layers)]
print(len(l1))
for j in range(len(scalars)):
  for i in range(len(l1)):
    if(i==0):
      weights[l1[i]]*=scalars[j]
      k=0
      for layers in model.layers:
        if(len(layers.get_weights())>0):
          layers.set_weights([weights[k],weights[k+1]])
          k+=2
      loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
      new_acc.append(acc*100)
    elif(i!=len(l1)-1):
      weights[l1[i-1]]/=scalars[j]
      weights[l1[i]]*=scalars[j]
      k=0
      for layers in model.layers:
        if(len(layers.get_weights())>0):
          layers.set_weights([weights[k],weights[k+1]])
          k+=2
      loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
      new_acc.append(acc*100)
    else:
      weights[l1[i-1]]/=scalars[j]
      weights[l1[i]]*=scalars[j]
      k=0
      for layers in model.layers:
        if(len(layers.get_weights())>0):
          layers.set_weights([weights[k],weights[k+1]])
          k+=2
      loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
      new_acc.append(acc*100)
      weights[l1[i]]/=scalars[j]

print((new_acc))
print(len(scalars))

"""## 5.6 Plotting graph for each layer for each scalar multiplied vs accuracy """

new_accs=[]
for i in range(num_layers):
  l2=[]
  for j in range(len(scalars)):
    l2.append(new_acc[num_layers*j+i])
  new_accs.append(l2)
for i in range(len(new_accs)):
  plt.plot(scalars, new_accs[i], label=f'layer {i+1}')
plt.xlabel('Scalar')
plt.ylabel('Accuracy')
plt.title('Effect on accuracy when weights are multiplied by scalers')
plt.legend()
plt.show()

"""## 5.7 Checking which layer is least and most sensitive to these changes and the average accuracy loss for each layers across all weights change """

for i in range(num_layers):
  for j in range(len(scalars)):
    new_accs[i][j]=abs(new_accs[i][j]-max_acc*100)

Average_loss_accs=[]
for i in range(num_layers):
  print(len(new_accs[i]))
  Average_loss_accs.append(sum(new_accs[i])/len(scalars))

print(Average_loss_accs)

print(sum(Average_loss_accs)/num_layers)

maxi=100000
mini=-10000
layer=-1
layer2=-1
for i in range(num_layers):
  if(maxi>Average_loss_accs[i]):
    maxi=Average_loss_accs[i]
    layer=i+1
  if(mini<Average_loss_accs[i]):
    mini=Average_loss_accs[i]
    layer2=i+1
print("The layer least sensitive to the changes is :" , layer)
print("The layer most sensitive to the changes is :" , layer2)

"""## 5.7 Defining scalars and changing weights of every layer by adding those scalars one by one"""

scalars1=[0.01,0.02,0.015,0.025,0.03,0.035,0.04,0.045,0.05]
new_acc1=[]
acc_change=[]

l1=[2*i for i in range(num_layers)]
print(len(l1))
for j in range(len(scalars1)):
  for i in range(len(l1)):
    if(i==0):
      weights[l1[i]]+=scalars1[j]
      k=0
      for layers in model.layers:
        if(len(layers.get_weights())>0):
          layers.set_weights([weights[k],weights[k+1]])
          k+=2
      loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
      new_acc1.append(acc*100)
    elif(i!=len(l1)-1):
      weights[l1[i-1]]-=scalars1[j]
      weights[l1[i]]+=scalars1[j]
      k=0
      for layers in model.layers:
        if(len(layers.get_weights())>0):
          layers.set_weights([weights[k],weights[k+1]])
          k+=2
      loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
      new_acc1.append(acc*100)
    else:
      weights[l1[i-1]]-=scalars1[j]
      weights[l1[i]]+=scalars1[j]
      k=0
      for layers in model.layers:
        if(len(layers.get_weights())>0):
          layers.set_weights([weights[k],weights[k+1]])
          k+=2
      loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
      new_acc1.append(acc*100)
      weights[l1[i]]-=scalars1[j]

"""## 5.8 Plotting graph for each layer for each scalar added vs accuracy """

new_accs1=[]
for i in range(num_layers):
  l2=[]
  for j in range(len(scalars1)):
    l2.append(new_acc1[num_layers*j+i])
  new_accs1.append(l2)
for i in range(len(new_accs1)):
  plt.plot(scalars, new_accs1[i], label=f'layer {i+1}')
plt.xlabel('Scalar')
plt.ylabel('Accuracy')
plt.title('Effect on accuracy when weights are added by scalers')
plt.legend()
plt.show()

"""## 5.9 Checking which layer is least and most sensitive to these changes and the average accuracy loss for each layers across all weights change    """

for i in range(num_layers):
  for j in range(len(scalars)):
    new_accs1[i][j]=abs(new_accs1[i][j]-max_acc*100)

Average_loss_accs=[]
for i in range(num_layers):
  print(len(new_accs1[i]))
  Average_loss_accs.append(sum(new_accs1[i])/len(scalars1))

print(Average_loss_accs)
print(sum(Average_loss_accs)/num_layers)

maxi=100000
mini=-10000
layer=-1
layer2=-1
for i in range(num_layers):
  if(maxi>Average_loss_accs[i]):
    maxi=Average_loss_accs[i]
    layer=i+1
  if(mini<Average_loss_accs[i]):
    mini=Average_loss_accs[i]
    layer2=i+1
print("The layer least sensitive to the changes is :" , layer)
print("The layer most sensitive to the changes is :" , layer2)

"""## 5.10 Changing weights of by random bit flipping method
From each layer some random weights are selected and theirs bits are flipped and accuracy is claculated . Same process is done for every layer .
"""

import struct 
import random

scalars2=[60,80,100,120,140]
new_acc2=[]

l1=[2*i for i in range(num_layers)]
for j in range(len(scalars2)):
  accuracies=[]
  for i in range(len(l1)):
    for p in range(scalars2[j]):
      k=random.randint(0,len(weights[l1[i]])-1)
      idx=random.randint(0,len(weights[l1[i]][k])-1)
      r_weight = weights[l1[i]][k][idx]
      r_weight = format(struct.unpack('!I', struct.pack('!f', r_weight))[0], '032b')
      r_bit_index = random.randint(0,31)
      r_bit = r_weight[r_bit_index]
      
      if(r_bit == '0'):
        r_weight = r_weight[:r_bit_index] + '1' + r_weight[r_bit_index + 1:]
      if(r_bit == '1'):
        r_weight = r_weight[:r_bit_index] + '0' + r_weight[r_bit_index + 1:]
      r_weight = struct.unpack('!f',struct.pack('!I', int(r_weight, 2)))[0]
      weights[l1[i]][k][idx]=r_weight
    model.set_weights(weights)
    print("layer ", i+1)
    print("no of weights changed: ",j+1)
    loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
    print("test accuracy {}".format(acc))
    accuracies.append(acc*100)
    print("\n")
    weights=original_weights
  new_acc2.append(accuracies)

"""## 5.11 Plotting graph for each layer for number of weights changed vs accuracy """

acc_graph=[]
for j in range(len(new_acc2[0])):
  a=[]
  for i in range(len(new_acc2)):
    a.append(new_acc2[i][j])
  acc_graph.append(a)

for i in range(len(acc_graph)):
  plt.plot(scalars2, acc_graph[i], label=f'layer {i+1}')
plt.xlabel('Number of weights changed')
plt.ylabel('Accuracy')
plt.title('Effect on accuracy when weights are changed by bit-flip')
plt.legend()
plt.show()

"""## 5.9 Checking which layer is least and most sensitive to these changes and the average accuracy loss for each layers across all weights change   """

average=[]
for i in acc_graph:
  average.append(sum(i)/len(i))
print(average)

maxi=100000
mini=-10000
layer=-1
layer2=-1
for i in range(num_layers):
  if(maxi>average[i]):
    maxi=average[i]
    layer=i+1
  if(mini<average[i]):
    mini=average[i]
    layer2=i+1
print("The layer least sensitive to the changes is :" , layer)
print("The layer most sensitive to the changes is :" , layer2)