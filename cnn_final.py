# -*- coding: utf-8 -*-
"""CNN_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZJbak7xql7pW-kpHvsiphptkAH_MVOuJ

Importing the required libraries.
"""

import numpy as np
import pandas as pd
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import pickle
from PIL import Image
import torchvision.datasets as dsets
import torchvision.transforms as transforms
import torch
import torch.nn as nn
import pickle
import os
import torch.nn.functional as F
import requests
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sn
import pandas as pd
torch.manual_seed(1)
from sklearn import preprocessing
from torch.utils.data import DataLoader, Dataset, Subset
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D,MaxPool2D,Dense,Flatten,Dropout,Input, AveragePooling2D, Activation,Conv2D, MaxPooling2D, BatchNormalization,Concatenate
from tensorflow.keras.callbacks import EarlyStopping, TensorBoard
from tensorflow.keras import regularizers, optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD
from tensorflow.keras.utils import to_categorical

"""# Importing Dataset and Preprocessing of data"""

IMAGE_SIZE = 32

mean, std = [0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]

composed_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
                                     transforms.RandomRotation(20), 
                                     transforms.RandomHorizontalFlip(0.1), 
                                     transforms.ColorJitter(brightness = 0.1, 
                                                            contrast = 0.1, 
                                                            saturation = 0.1), 
                                     transforms.RandomAdjustSharpness(sharpness_factor = 2,
                                                                      p = 0.1), 
                                     transforms.ToTensor(),  
                                     transforms.Normalize(mean, std), 
                                     transforms.RandomErasing(p=0.75,scale=(0.02, 0.1),value=1.0, inplace=False)])


composed_test = transforms.Compose([transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),
                                    transforms.ToTensor(),
                                    transforms.Normalize(mean, std)])

train_dataset =  dsets.CIFAR10(root='./data', train=True, download=True, transform = composed_train)
validation_dataset = dsets.CIFAR10(root='./data', train=False, download=True, transform = composed_test)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
assert x_train.shape == (50000, 32, 32, 3)
assert x_test.shape == (10000, 32, 32, 3)
assert y_train.shape == (50000, 1)
assert y_test.shape == (10000, 1)

# Normalizing
x_train=x_train/255
x_test=x_test/255

#One hot encoding
y_train_cat=to_categorical(y_train,10)
y_test_cat=to_categorical(y_test,10)

print('Shape of x_train is {}'.format(x_train.shape))
print('Shape of x_test is {}'.format(x_test.shape)) 
print('Shape of y_train is {}'.format(y_train.shape))
print('Shape of y_test is {}'.format(y_test.shape))

"""# Defining Base Model"""

model1 = Sequential()
model1.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
model1.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model1.add(MaxPooling2D((2, 2)))
model1.add(Dropout(0.2))
model1.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model1.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model1.add(MaxPooling2D((2, 2)))
model1.add(Dropout(0.2))
model1.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model1.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model1.add(MaxPooling2D((2, 2)))
model1.add(Dropout(0.2))
model1.add(Flatten())
model1.add(Dense(1024, activation='relu', kernel_initializer='he_uniform'))
model1.add(Dense(1024, activation='relu', kernel_initializer='he_uniform'))
model1.add(Dense(1024, activation='relu', kernel_initializer='he_uniform'))
model1.add(Dropout(0.2))
model1.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))
model1.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))
model1.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))
model1.add(Dropout(0.2))
model1.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))
model1.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))
model1.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))
model1.add(Dropout(0.4))
model1.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))
model1.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))
model1.add(Dropout(0.2))
model1.add(Dense(10, activation='softmax'))
# compile model
opt = SGD(lr=0.01, momentum=0.9)
model1.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

model1.summary()

"""Training on defined model."""

history1=model1.fit(x_train,y_train_cat,epochs=15,validation_data=(x_test,y_test_cat))

"""Evaluating the trained model."""

evaluation = model1.evaluate(x_test, y_test_cat)
print('Test Accuracy: {}'.format(evaluation[1]))

"""# Changing the weights of trained model"""

original_w = model1.get_weights()
original_w

"""Storing number of hidden layer defined in the base model"""

num_layers=12

"""Changing the weights of model starting from last layer to first layer and accuracy is stored in accuracys."""

accuracys=[]
x=num_layers
for i in range(len(original_w)-2,len(original_w)-1-2*num_layers,-2):
  accuracy=[]
  print("Layer :",x)
  for scalar in [.25, 0.5, .75, 0.9, 1.1,1.25,1.5,1.75,2.]:
    new_w=[]
    for w in range(len(original_w)):
      if(w==i):
        new_w.append(original_w[w]*scalar)
      else:
        new_w.append(original_w[w])
    model1.set_weights(new_w)
    evaluation = model1.evaluate(x_test, y_test_cat)
    print("Scallar {} --> test accuracy {}".format(scalar,evaluation[1]))
    accuracy.append(evaluation[1])
    model1.set_weights(original_w)
  x-=1
  model1.set_weights(original_w)
  accuracys.append(accuracy)

"""Layer-wise accuracy across all the change in weights are plotted. Scalars contains the scalars used to change the weights."""

scalars=[ .25, 0.5, .75, 0.9, 1.1,1.25,1.5,1.75,2.]
for i in range(len(accuracys)):
  plt.plot(scalars, accuracys[i], label=f'layer {num_layers-i}')
plt.xlabel('Scalar')
plt.ylabel('Accuracy')
plt.title('Effect on accuracy when weights are multiplied by scalers')
plt.legend()
plt.show()

"""Average stores the average accuracy of each layer across all weight change."""

average=[]
for i in accuracys:
  average.append(sum(i)/len(i))
print(average)

"""Again changing the weights by adding impurities into the weights."""

accuracys=[]
x=num_layers
for i in range(len(original_w)-2,len(original_w)-1-2*num_layers,-2):
  accuracy=[]
  print("Layer :",x)
  for scalar in [.25, 0.5, .75, 0.9, 1.1,1.25,1.5,1.75,2.]:
    new_w=[]
    for w in range(len(original_w)):
      if(w==i):
        new_w.append(original_w[w]+scalar)
      else:
        new_w.append(original_w[w])
    model1.set_weights(new_w)
    evaluation = model1.evaluate(x_test, y_test_cat)
    print("Scallar {} --> test accuracy {}".format(scalar,evaluation[1]))
    accuracy.append(evaluation[1])
    model1.set_weights(original_w)
  x-=1
  model1.set_weights(original_w)
  accuracys.append(accuracy)

for i in range(len(accuracys)):
  plt.plot(scalars, accuracys[i], label=f'layer {num_layers-i}')
plt.xlabel('Scalar')
plt.ylabel('Accuracy')
plt.title('Effect on accuracy when weights are added by scalers')
plt.legend()
plt.show()

average=[]
for i in accuracys:
  average.append(sum(i)/len(i))
print(average)

"""history=model.fit(x_train,y_train_cat,epochs=10,validation_data=(x_test,y_test_cat))"""

accuracys=[]

import random
import numpy as np
import struct
x=num_layers - 1
new_acc = []
indexes = []
weights = original_w.copy()
for i in range(len(original_w)-2,len(original_w)-1-(2*x),-2):
   indexes.append(i)
l1=indexes[::-1]
for i in range(len(l1)):
  
  for j in range(2,3,2):
    for k in range(len(weights[l1[i]])):
      
      idx=random.randint(0,len(weights[l1[i]][k])-1)
      r_weight = weights[l1[i]][k][idx]
      r_weight = format(struct.unpack('!I', struct.pack('!f', r_weight))[0], '032b')
      r_bit_index = random.randint(0,31)
      r_bit = r_weight[r_bit_index]
      
      if(r_bit == '0'):
        r_weight = r_weight[:r_bit_index] + '1' + r_weight[r_bit_index + 1:]
      if(r_bit == '1'):
        r_weight = r_weight[:r_bit_index] + '0' + r_weight[r_bit_index + 1:]
      r_weight = struct.unpack('!f',struct.pack('!I', int(r_weight, 2)))[0]
      weights[l1[i]][k][idx]=r_weight
  model1.set_weights(weights)
  print("no of weights changed: ",j)
  # acc = model1.evaluate(x_test, y_test)
  # print("test accuracy {}".format(acc[1]))
  evaluation = model1.evaluate(x_test, y_test_cat)
  print("test accuracy {}".format(evaluation[1]))
  new_acc.append(evaluation[1]*100)
  weights=original_w
  print("\n")

"""Setting back the original weights and checking if we get the same accuracy"""

acc_graph=[]
for j in range(len(accuracys[0])):
  a=[]
  for i in range(len(accuracys)):
    a.append(accuracys[i][j])
  acc_graph.append(a)

numberofweightchange=[50,75,100,125,150]
for i in range(len(acc_graph)):
  plt.plot(numberofweightchange, acc_graph[i], label=f'layer {len(acc_graph)-i-1}')
plt.xlabel('Number of weights changed')
plt.ylabel('Accuracy')
plt.title('Effect on accuracy when weights are changed by bit-flip')
plt.legend()
plt.show()

average=[]
for i in acc_graph:
  average.append(sum(i)/len(i))
print(average)